\chapter{Design}\label{design}

Significant design work has been performed for this system at both the architectural level and user interface. Interface design is discussed in section \ref{screen_design}. 

I have chosen to implement a client server model for the system architecture, as this offers a strong separation into multiple loosely coupled modules. Given a client server architecture, and a desire for centralised data storage between multiple users a browser based approach was chosen. Using existing browsers leverages significant work done on secure network communications between client and server, as well as significant effort in sand-boxing (isolating from the rest of the computer) in-browser applications. This saves significant time and effort on implementing communication protocols and security. 

As there are relatively few programming languages available for use within the browser this significantly simplified the choice of languages and tool sets. See section \ref{langs}.

\begin{figure}[tbh]
\fbox{\parbox[b]{.98\linewidth}{
\vskip 0.5cm
\centering \includegraphics[scale=0.4]{blocks.png}
\vskip 0.5cm}}
\caption{\protect\label{spiral_plan}Block diagram showing proposed architecture of system. Note complete separation between server and browser. These may reside on separate machines.}
\end{figure}

Inside the browser there are three major components from an object oriented perspective. The view contains what is actually displayed on screen, this is implemented in HTML with SVG (Scalable Vector Graphics) for graphical components. The model stores the data and handles requests to the server as necessary. The controller is responsible for implementing user interaction with the system.

Communication between the client and server is handled through standardised protocols. The set of requests permissible is still to be defined. The server software is responsible for serving data to the client, performing data aggregation, and limited data mining. In order to provide data, the server communicates with the data-store. This communication is likely to take place through SQL to a database, though a modular design could permit creation of other storage interface modules.

Data aggregation is performed server side for two reasons. In larger networks, or over longer time periods, many megabytes of logs can be created, while text is highly compressible this still creates significant network overhead to transfer. Further, there are unanswered questions about how well the client will perform with potentially millions of events to manipulate. Slow data transfers or interaction due to processing time would be a significant usability hurdle. Performing data aggregation on the server bypasses both of these limitations. 

\section{User interface design}\label{screen_design} 
  
I have chosen to adopt a time based approach for the visualisation elements, as access attempt logs have a very strong time component. Some forms of unusual behaviour are evidenced only by unusual times for access attempts, or unusual durations of access for example. Most existing visualisations do not give much emphasis to the time relation between access attempts, preferring to focus on the links between source and destination addresses.

Two major approaches have been considered for displaying log entries.
Spiral view is an approach for displaying time series data mapped to a spiral, see figure \ref{spiral} where a straight line drawn from the outer edge to the center shows the same time at each level of the spiral \cite{bertini2007spiralview, chin2009visual}
These systems appear to be highly effective at displaying time series data in a fashion that supports easy detection of repeated patterns. 

\begin{figure}[tbh]
\fbox{\parbox[b]{0.98\linewidth}{
\vskip 0.5cm
\centering \includegraphics[scale=0.71]{spiral_plan.png}
\vskip 0.5cm }}
\caption{\protect\label{spiral_plan}Drawing showing proposed layout using spiralview for main focus. Black shading shows selection of an event with highlighting of related events.}
\end{figure}
 
However serious flaws in the results(\cite{chin2009visual}) of usability tests presented have lead to the rejection of this method. The error in question shows a table of results that directly contradict claims made in the text about how well spiralview supports the detection of patterns. The claims contradicted in the work are those directly bearing on the uses I had intended for the spiral layout. As this is a 300 hour project, I will not be attempting to repeat their work to clarify their findings. I attempted to contact the authors of the paper, and have as yet received no response.

This contradiction casts doubt on the ability of the spiralview model to effectively support data hiding, navigation and manage potential for information overload. These aspects are key functional requirements for this project \ref{reqs}. Due to this doubt, and lack of response from the authors when clarification was requested, I have chosen not to use the spiral model. 

\begin{figure}[tbh]
\fbox{\parbox[b]{.98\linewidth}{
\vskip 0.5cm
\centering \includegraphics[scale=0.71]{lines.png}
\vskip 0.5cm}}
\caption{\protect\label{lines}Drawing showing proposed layout using multiple time lines for main focus. Black shading shows selection of an event, with highlighting related events.}
\end{figure}

Using block representation of data arranged linearly along a time axis.
Initial designs called for using a stacked representation where each layer represented twice the time of the previous layer. This is proposed to show patterns as an element that repeats a growing number of times in each layer.
Tufte's work on small multiples \cite{tufte1983visual} suggests keeping each level of the stack to represent the same amount of time, with differing start and end points. Ie: if the range is 6 hours, the top bar starts at 0600, and runs to 1200, the one below it from 0000 to 0600..  and so on. This would show patterns repeating with a period less than the time range by showing in multiple bars. See Figure \ref{lines}. The timeline visualisation chosen is familiar to most people due to wide useage. Familiarity with this model supports. 

I have been forced to consider data hiding techniques, as networks can become extremely busy, producing sufficient activity to overwhelm the user's ability to absorb information and detect meaningful patterns in the clutter. As I have chosen a time series based approach to displaying the data, I have chosen to use time binning to aggregate entities. This approach is extremely simple, with all entries in a short time period displayed as a single entity, with icons indicating some simple features of the hidden data. Such features include superuser accesses, abnormal numbers of failed access attempts, abnormally large numbers of access attempts, abnormal login locations for a user, abnormal login times for a user. Each time bin can be zoomed in on, allowing the user to see greater detail within the bin. For extremely busy systems and longer time periods there may be multiple levels of binning in play to aggregate sufficiently. This scheme reduces the visual complexity, while allowing easy access to detailed information about each incident. 

These last are the most complicated flags, as they require creating a profile of each user's access times over repeated access attempts. This complexity can produce false positives while the system is learning a new user's habits. and may be tripped up by a legitimate change in user habits.

\section{Parser}\label{parser}
Not really a lot to say about the parser design. it was largely dictated by the structure of log files.
Discuss log file structure here? not really anywhere else to discuss it.

Mar 16 08:25:22 app-1 sshd[4884]: Server listening on :: port 22. \\
Mar 16 08:25:22 app-1 sshd[4884]: error: Bind to port 22 on 0.0.0.0 failed: Address already in use. \\
Apr 19 05:55:20 app-1 sshd[12996]: Accepted password for root from 219.150.161.20 port 55545 ssh2 \\
Apr 19 05:55:20 app-1 sshd[12997]: Invalid user pauline from 219.150.161.20 \\
Apr 19 05:55:21 app-1 sshd[12990]: Failed password for root from 219.150.161.20 port 54890 ssh2 \\
Apr 20 00:00:51 app-1 sshd[24442]: subsystem request for sftp \\
Jun 8 01:03:34 machine0 sshd[1796]: Received disconnect from 38.165.101.19: 11: Bye Bye \\

Exemplars of different log entry types.


All log entries contain a partial timestamp, server name, service name and process id. These fields are required by the syslog format used by openSSH. The remainder of the message is a free text field. Each has different metadata to break down. IE:  a connection attempt has information about authentication method, username, source address, and status. Whereas a disconnection message contains a code, and 
source IP.

why do clustering here? - not really anywhere else to do it. Clustering in the server could only include data in current selection. This would result in instability in results (ie: if select one set of dates, a login is normal, select a different set and it's classified as abnormal). 

\section{Database} 

Why choose a RDBMS in the first place? maybe put this in the design overview section. Basically fell into RDBMS system because it's something I was somewhat familiar with in the beginning, and sufficiently appropriate.

Advantages of RDBMS systems, handle concurrency issues natively so long as transaction semantics are used, efficient filtering on attribute values. NoSQL datastores frequently do not handle concurrent access issues. This would require a significant redesign of both server and parser to handle concurrent updates. Concurrent reads present no issue. 

Schema for log events is represented in a single table \ref{entry_schema}. As there are several subtypes of log events, with differing metadata \ref{parser}, there are many nullable values in the event table.
\begin{table}[tbh]
\centering
\begin{tabular}{l || l | l | p{0.5\textwidth}}
Column & Datatype & Nullable & Conditions \\ \hline
id & Int & No & Autoincremented primary key \\
timestamp & Bigint & No & Timestamp stored as milliseconds since unix epoch. \\
server & int & No & Foreign key, linking to server table. \\
connid & int & No & Badly named, Process id for handling sshd process. \\
reqtype & Enum & No & Tag field, used to identify which of the possible event types this row represents. (connect, disconnect, subystem request, invalid username, other) \\
authtype & Enum & Yes & What authentication method was used. (password, hostbased, key, gssapi, or none) \\
status & Enum & Yes & Did the connection attempt succeed? (accepted, failed) \\
user & int & Yes & Foreign key to user table. \\
source & char(45) & Yes & text representation of Ipv4 or IpV6 address the request originated from. \\
port & smallint & Yes & Port the request was sent from. \\
subsystem & Enum & Yes & Which subsystem was requested. (sftp, scp) \\
code & int & Yes & disconnect code, not actually used. \\
isfreqtime & int & Yes & Reference to information about time cluster this request belongs to, or null. \\
isfreqloc & int & Yes & Reference to information about location cluster this request belongs to, or null. \\
rawline & text & No & Unprocessed line, exactly as read from logfile. \\
\end {tabular}
\caption{Schema for entry table}
\label{entry_schema}
\end{table}

Several considerations were involved in creating this table schema. As there are multiple types of log entry considered by the system, database design suggests three alternative approaches to table layout to handle this. A single unified table, with many nullable elements, A table storing all columns common to each entry type (Non nullable columns in \ref{entry_schema}) with additional tables for each subtype storing only information unique to that entry type. Or a table for each entry type with all attributes used in that subtype, including shared attributes.

It is usually considered good design to avoid large numbers of nullable attributes, as this complicates integrity checking and in many cases forces checks to be carried out at the application level, or in complex stored procedures. However, performance considerations have forced the use of this schema design. In practice, there are very few queries to the database that do not involve a range select on timestamp (all rows with timestamp between x and y) for all subtypes of entry. In both multitable schema designs, this must be implemented with multiple joins (5 or 6). As this tool is intended to scale to many millions of events, and joins are the least efficient database operation, nonfunctional requirements strongly suggest avoiding joins where possible. This has lead directly to the table schema shown \ref{entry_schema}. Note that the server table stores some basic metadata about each server, this is currently not utilized, though may be in future extensions.

The user table is a holdover from an older design<reword, it's actually necessary to link tables together.>, which proposed storing some metadata about users, such as validity, role, and login patterns. This would have eliminated the need for the invalid user request type, as this is just a failed connection, due to invalid user. This design was not pursued, due to issues with time. In the honeynet dataset, users are created and destroyed. This results in previously invalid usernames becoming valid. With this design, that would require multiple entries for each user, as an invalid user linked against an entry on date x, does not become valid when the user is created on date y (y after x).

Users may also have several independent frequent login times and locations, ie: Some IT staff may frequently log in from home as well as in the office. This resulted in frequent time and location information being split into four separate tables. <include table schema here?>

\section{Server}
why two layer architecture - ease of shifting datastore, changes there are isolated from server and thus web app, may find more appropriate datastore for semistructured log data in future. - why not three?
Only one servlet has any data processing outside of database, so third layer usually unneccessary, and overkill in the one case where some processing needed.

servlet structure largely dictated by servlet API's and choice of tomcat as server.  not a lot of design work here. different request types should be handled by specialised servlets. Collection of servlets and static resources forms a web-app.

\section{Requirements}
discuss here some thoughts as to what requirements were met, and which were abandoned for time limitations.
requirements for log parsing limited to SSHD logs, not limited to one ssh demon.
network context dropped.



