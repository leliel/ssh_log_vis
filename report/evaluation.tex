\chapter{Evaluation}\label{eval}

Evaluation of the tool produced in this project served to determine if there is sufficient merit to the idea to justify further development work on the tool. If the evaluation determines that there is sufficient potential to justify the development work, future work can evaluate the effectiveness of the design in a more rigorous manner. I have chosen to perform an exploratory evaluation due to time limitations on this project \cite{Ellis:2006:EAU:1168149.1168152}. It is not possible to achieve a representative sample of sufficient size to have predictive power in the time available. Two factors limit the ability to achieve a representative sample in the time available, taking part in the experiment requires between 1 and 1.5 hours. a further 1 to 1.5 hours is required to grade and analyse the data. Further, there is a relatively limited pool of potential participants as the tool is targeted at security experts. This limitation strongly suggests against attempting to demonstrate conclusively the usefulness and usability of the system. Small, exploratory evaluations have advantages in cost and time required. With small numbers of participants, the evaluation can be carried out in a short time. These exploratory evaluations can also be useful to discard non-viable approaches before significant development effort has been spent. Care should be taken to avoid evaluating too early however, as the unfinished nature of the product can distort results. 

Ethics approval has been granted for this experimental design.<ref appendix> 

\section{Evaluation Design}

Evaluation was performed by means of a qualitative user study, with 6 expert participants. Two datasets were used, representing different sizes and complexities of system. Each participant was presented with 4 tasks to complete for each dataset. Time taken and accuracy was recorded for each task, along with a 3 question usability survey. 

\subsection{Users}

As this tool targets domain experts in the security domain, I have recruited participants from the security industry, both on campus and off. I have pitched the experiment to students enrolled in NWEN405, which deals with computer security as a secondary source of participants.
--Expand this section, leave full personae in appendix.
4 participants from industry, 3 at VUW
All four professional participants had security issues as at least a portion of their day jobs, and could be expected to be involved in log analysis tasks on a regular basis.
2 students.  Both students had strong security and networking backgrounds.

\subsection{Datasets}\label{data}

Two datasets have been acquired for use in this experiment. Both datasets represent real systems exposed to the public internet. 
\begin{enumerate}
\item{Honeynet Forensic challenge 10 dataset \cite{forensic10}. This dataset is anonymised and public domain. The data presented here covers a single server, with 35K log entries covering March 16 to May 2}
\item{Anonymised logs from the Engineering and Computer Science network at Victoria University. This dataset covers three servers, for two disjoint weeks and 74K log entries.}
\end{enumerate}

While both datasets were anonymized, anonymization procedures were different. The honeynet dataset is publically available in an anonymized form \cite{forensic10} whereas the ECS dataset was recorded from internet facing servers in the ECS network. The latter dataset required anonymization of both usernames and IP addresses. IP addresses were anonymized with cryptoPAN <ref> a prefix preserving IP address anonymization tool extended with support for IPv6 addresses. Usernames were anonymized through a very simple scheme, where each username was replaced with the string "user" + a unique number. ie: user4 is a different person to user5.

The ECS dataset was altered, to introduce a successful scattergun attack on one server. ThIs was introduced, as there were no naturally occuring successful attacks.  

\subsection{Setup}
Participants were provided with one of two machines
\begin{itemize}
\item{Dell Optiplex 9010 with an i7-3770 CPU and 8Gb of ram running Arch linux 3.7.5}
\item{Dell Optiplex GX760 with a Core2 Duo CPU and 4GB ram, running Arch linux 3.7.5}
\end{itemize}
Both systems had chrome 26.0.1410.63 as the browser.

Different hardware was used, as I was not able to secure the same room for all participants. 
While there are significant differences in the hardware capabilities, I do not believe that these are significant
as the design of the tool does all memory and computationally intensive work server side, with only a few kilobytes of data transferred for each view. This results in a relatively small memory footprint, and limited resource requirements for the client computer.
More significant is the consistent OS and browser versions, as javascript behaviour can be influenced strongly by browser implementation. Though this is relatively rare for javascript behaviour to change significantly between major browser versions, this is not unknown. By ensuring both systems used the same OS and browser, this possible source of error is eliminated.

The experiment was conducted in a quiet lab, with only the experimenter and subject present. Each question was allocated an 8 minute maximum, based on test runs with a supervisor. Audio recordings were made as a record of events during the experiment as an addition to handwritten notes. 

\subsection{Procedure}
Participants were given up to ten minutes to familiarise themselves with the tool, and how it behaves. After familiarisation, participants were asked to answer four questions about each of two datasets. For each of the 8 combinations a brief questionnaire was completed indicating opinions about the tool's performance in the task\cite{lewis1995ibm}.
Questions were presented to users in randomised order. This was done in order to avoid learning effects distorting results for the first question.

\begin{enumerate}
\item{Find an instance of a successful brute force attack on root.}
\item{Find an instance of a successful scattergun attack.
(an instance where the attacker attempts many common username/password pairs at random).}
\item{Find an instance of a legitimate user logging in from an abnormal location.}
\item{Find an instance of a legitimate user logging in at an abnormal time.}
\end{enumerate}

Questions 1 \& 2 are based on the most commonly found attack signatures in SSH logs. with many botnets and automated systems carrying out brute force or scattergun attacks against any IP address responding to connection requests. As these attacks are very common and can lead to serious compromises, as shown in the Honeynet dataset, determining success or failure of such attacks is a core function of any log analysis tool. 

Questions 3 \& 4 are based in finding anomalous behaviour by legitimate users. Anomalous behaviour by legitimate users can be an indication that their account has been compromised, or that they are attempting to carry out actions that would not be permitted in usual usage. <Badly worded. I mean they might be trying to compromise the system or data stored there.>. 

Tasks 1 through 4 were based on questions 1 \& 2, with tasks 1 \& 4 using the ECS dataset, and tasks 2 \&3 using the Honeynet dataset.
Tasks 5 through 8 were based on questions 3 \& 4, with tasks 5 \& 7 using the ECS dataset, and tasks 6 \& 8 using  the Honeynet dataset. 

Timing and accuracy for each task was recorded. 
Time was measured manually, by means of a stopwatch. While stopwatches are hard to use for subsecond accuracy, this is not required for a study of this type. Errors in timing on the order of 5 seconds are acceptable.

For each task a date, time, source IP and where applicable username involved were recorded. This combined with the dataset provides sufficient information to allow checking of answers for accuracy from the raw logs, database directly, or using the tool. 

\subsection{Threats to validity}
 Students are less ideal, as their experience and domain knowledge are more limited than those that have been working in the industry for some time.
Dataset size - both in 10K order of magnitude, this limits our ability to speculate about how the tool will scale to 100K+ sizes. 
Small number of participants - unable to control effectively for demographic variations, or ensure 

\section{Results}

Each Task was graded for accuracy, and time taken to complete the task was measured.
\begin{table}[tbh]
\centering
\begin{tabular}{l|*{12}{l|}}
Task & 
\multicolumn{2}{|c|}{Participant 1} & 
\multicolumn{2}{|c|}{Participant 2} & 
\multicolumn{2}{|c|}{Participant 3} & 
\multicolumn{2}{|c|}{Participant 4} & 
\multicolumn{2}{|c|}{Participant 5} & 
\multicolumn{2}{|c|}{Participant 6} \\
\hline
Task 1 & 3:05 & \cmark & 5:46 & \xmark & Invalid & \xmark & - & \xmark & - & \xmark & - & \xmark \\
Task 2 & - & \xmark & - & \xmark & 2:26 & \cmark & 2:09 & \cmark & 3:55 & \cmark & 3:08 & \cmark \\
Task 3 & 5:10 & \cmark & - & \xmark & 4:18 & \cmark & 4:51 & \cmark & 8:00 & \cmark & 2:20 & \xmark \\
Task 4 & - & \xmark & - & \xmark & 4:18 & \cmark & 3:18 & \xmark & - & \xmark & - & \xmark\\
Task 5 & 1:09 & \cmark & 1:22 & \cmark & 1:07 & \cmark & 0:39 & \cmark & 1:04 & \cmark & 1:10 & \cmark \\
Task 6 & 0:31 & \cmark & 1:07 & \cmark & 1:02 & \cmark & 1:13 & \xmark & 1:00 & \cmark & 2:15 & \cmark \\
Task 7 & 0:45 & \cmark & 1:59 & \cmark & 0:42 & \xmark & 0:44 & \cmark & 2:06 & \cmark & - & \xmark \\
Task 8 & 0:32 & \cmark & 1:10 & \xmark & 1:32 & \xmark & 1:07 & \cmark & 1:15 & \cmark & 0:55 & \cmark \\
\end{tabular}
\caption{Time taken to complete each tasks}
\label{res_times}
\end{table}

Observation of participants working on tasks 1 through 4, and participant comments suggest that difficulty in navigating the timeline was a significant issue for carrying out these tasks.
Task 1 involved finding a brute force attack which compromised root on the ECS dataset. There was no such attack present in the dataset. It's well known that demonstrating the absence of something can be significantly harder than the presence.<justify this>. I believe the combination of navigational difficulties with increased task difficulty is the cause of the very high failure rate for this task, with only one participant successful.

Task 4 involved participants looking for a successful scattergun attack in the ECS dataset. why was this so hard? Poor navigation support caused problems, relatively small attack signature easily swamped in other data.

Tasks 2 \& 3 were to find a brute force and scattergun attack respectively on the Honeynet dataset.
There were multiple successful brute force attacks on the Honeynet dataset, and scattergun attacks with much larger attack signatures (higher number of attempts). These questions were quickly and reliably answered by all but one participant. Poor accuracy, and significantly slower times with ECS data coupled with observations of participants attempting these tasks suggest that navigation difficulties and limited filtering options were a greater issue in the more complicated dataset, with smaller attack signatures, and greater noise. 

The introduced scattergun attack in the ECS dataset had a relatively small attack signature, with only 204 log entries involved, of 4.7K entries for that day. Some successful attacks on honeynet had similar numbers of access attempts, though often a much higher proportion of invalid or failed attempts for a given day. 

There were many more legitimate access attempts to the ECS network, and logging of disconnect messages introduced further noise to the system. 
<check actual results, some successful root attacks on honeynet had similar sized signatures, some much larger, though with more noise for larger datasets.>
this needs more analysis.

Participant 2 had a great deal of difficulty in identifying brute force and scattergun attacks on both datasets. Feedback and observation of the participant in action suggest severe difficulties with navigation, combined with the lack of ability to hide all attempts from a specified set of IP addresses caused significant difficulties for this participant.

Participant 2 commented that in the normal course of investigating such incidents using tools such as grep, he would build up a blacklist of IP's to hide from results as they were fully investigated and discarded.
The tool as currently implemented does not support this. Participant 2 has significantly more experience in analysing sshd logs using traditional tools where stronger filtering tools are available, such as regular expressions. 

\section{Discussion} 

Analysis of the results of this user evaluation suggest that there are several obvious weaknesses in the tool as currently implemented. 

These break down into three major areas, Navigation, Filtering, and information hiding.
Navigation difficulties were experienced by a majority of users, where abrubt transitions between zoom levels lead to loss of context, and difficulty building up a mental map of the timeline. 
Most users demonstrated improved navigation as they became more familiar with the tool. these issues could be addressed in two major ways. Showing a miniature zoomed out view as a context for the main view could assist users in maintaining context. 
Animating transitions may also assist users to build up a mental map of the log. Improvements to navigation would support nonfunctional requirements 1 through 3 (\ref{reqs}). 

Analysis of the difficulties experienced by participant 2, and suggestions from other participants suggest several new filtering options would significantly enhance ability to deal with potential information overload. 
Implementing IP blacklisting would be extremely useful for some users, as it would support a n interaction model where one IP is fully investigated, then hidden, and the process repeated until all suspicious IP's have been investigated. 

One participant suggested allowing filtering by authentication method. This is strongly supported by the difficulties users had in Task 1, as on multiple occasions, a successful root login would occur mixed in with many failed attempts. This successful login would be from a different IP address, using an authentication method not amenable to brute forcing, such as host-based authentication. Allowing filtering by authentication method, would assist users in avoiding this pitfall. <needs rewording to tie in functional and nonfunctional requierements.>


\begin{enumerate}
\item{Improved navigation through timeline. Users complain that it is easy to get lost when zooming. Animations may ease this transition. (possibly animate zooming in by expanding target bin to fill timeline, then chunking into new bins?)}
\item{Improved filtering of results by IP. several extra features are requested here, with the ability to filter by subnet, as well as hiding IP's (inverse of IP of Interest filter.)}
\item{Ability to filter connections by authtype, ie : excluding hostbased when looking for a successful brute force attack.}
\item{Improve performance of abnormal time and location detection, reduce false positives and spurious results by suppressing alerting on invalid or failed attempts. This will significantly cut down the number of abnormalities reported, reducing the false positive rate significantly.}
\item{seperate disconnection messages into their own group, and allow hiding as done for successful and failed connection attempts. (leave as one colour?)}
\end{enumerate}

include comparison with goals here.. how well did we meet them?
\begin{enumerate}
\item{Effective use of information hiding to prevent information overload.}
\item{Prevent masking of important data in noise or hiding. failed, more work required on filtering and clustering}
\item{Provide strong filtering and highlighting options. weaker than ideal, more work needed, performance dependent on exploration style and ability to maintain mental map of data.}
\item{Show surrounding context for anomalous accesses.}
\item{GeoIP support to add context to login attempts. geoIP results used in clustering algorithm, but not directly displayed.}
\item{Allow the user control over which machine is monitored at any given time. -included as dropdown selector, may have some scale issues for networks with many machines to monitor. no ability to select multiple machines currently, though dropdown easily expanded to allow this.}
\item{Show network context for currently monitored machine. dropped for lack of time}
\item{Performance capable of handling millions of events without perceptible delays. - untested, but unlikely due in part to network latency and javascript performance issues. did not have sufficiently large dataset to test performance with more than $10^3$ magnitude.}
\item{Extensible log parsing: don't prevent extension to other similar log types.}
\end{enumerate}

\section{Future work}
How do we go about addressing the weaknesses discussed above?.
can they be addresssed?
address known bugs.
address discovered weaknesses
 - add more filter types
 - add mechanism for excluding addresses from results.
 - refine clustering algorithm to reduce false positives. 
 - animate transitions to assist navigation.
maybe some work on performance?
critical - once bugs and new features addressed, perform a more complete evaluation
higher n, with actual statistical power, and much more variety of datasets. allow more training time, and some learning questions.