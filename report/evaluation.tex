\chapter{Evaluation}\label{eval}

Evaluation of the tool produced in this project served to determine if there is sufficient merit to the idea to justify further development work on the tool. If the evaluation determines that there is sufficient potential to justify the development work, future work can evaluate the effectiveness of the design in a more rigorous manner. I have chosen to perform an exploratory evaluation due to time limitations on this project \cite{Ellis:2006:EAU:1168149.1168152}. It is not possible to achieve a representative sample of sufficient size to have predictive power in the time available. Two factors limit the ability to achieve a representative sample in the time available, taking part in the experiment requires between 1 and 1.5 hours. a further 1 to 1.5 hours is required to grade and analyse the data. Further, there is a relatively limited pool of potential participants as the tool is targeted at security experts. This limitation strongly suggests against attempting to demonstrate conclusively the usefulness and usability of the system. 

Ethics approval has been granted for this experimental design.<ref appendix>

\section{Evaluation Design}

Evaluation was performed by means of a qualitative user study, with 6 expert participants. Two datasets were used, representing different sizes and complexities of system. Each participant was presented with 4 tasks to complete for each dataset. Time taken and accuracy was recorded for each task, along with a 3 question usability survey. 

\subsection{Users}

As this tool targets domain experts in the security domain, I have recruited participants from the security industry, both on campus and off. I have pitched the experiment to students enrolled in NWEN405, which deals with computer security as a secondary source of participants.
--Expand this section, leave full personae in appendix.
4 participants from industry, 3 at VUW
All four professional participants had security issues as at least a portion of their day jobs, and could be expected to be involved in log analysis tasks on a regular basis.
2 students.  Both students had strong security and networking backgrounds.

\subsection{Datasets}\label{data}

Two datasets have been acquired for use in this experiment. Both datasets represent realistic systems exposed to the public internet. 
\begin{enumerate}
\item{Honeynet Forensic challenge 10 dataset \cite{forensic10}. This dataset is anonymised and public domain. The data presented here covers a single server, with low traffic (9MB covering mar 16 to may 2)}
\item{Anonymised logs from the Engineering and Computer Science network at Victoria University. This dataset is significantly larger, covering 3 servers and 2 weeks of time on a high traffic network. }
\end{enumerate}

While both datasets were anonymized, anonymization procedures were different. The honeynet dataset is publically available \cite{forensic10} whereas the ECS dataset was recorded from internet facing servers in the ECS network. The latter dataset required anonymization of both usernames and IP addresses.
IP addresses were anonymized with cryptoPAN <ref> a prefix preserving IP address anonymization tool, extended with support for IPv6 addresses. Usernames were anonymized through a very simple scheme, where each username was replaced with the string "user" + a unique number. ie: user4 is a different person to user5.

\subsection{Setup}
Participants were provided with one of two machines
\begin{itemize}
\item{Dell Optiplex 9010 with an i7-3770 CPU and 8Gb of ram running Arch linux 3.7.5}
\item{Dell Optiplex GX760 with a Core2 Duo CPU and 4GB ram, running Arch linux 3.7.5}
\end{itemize}
Both systems had chrome 26.0.1410.63 as the browser.

The experiment was conducted in a quiet lab, with only the experimenter and subject present. Each question was allocated an 8 minute maximum, based on test runs with a supervisor.

\subsection{Procedure}
Participants were given up to ten minutes to familiarise themselves with the tool, and how it behaves. After familiarisation, participants were asked to answer four questions about each of two datasets. For each of the 8 combinations a brief questionnaire was completed indicating opinions about the tool's performance in the task\cite{lewis1995ibm}.
Questions were presented to users in randomised order. This was done in order to avoid learning effects distorting results for the first question.

\begin{enumerate}
\item{Find an instance of a successful brute force attack on root.}
\item{Find an instance of a successful scattergun attack.
(an instance where the attacker attempts many common username/password pairs at random).}
\item{Find an instance of a legitimate user logging in from an abnormal location.}
\item{Find an instance of a legitimate user logging in at an abnormal time.}
\end{enumerate}

Tasks 1 through 4 involved questions about finding brute force and scattergun attacks on both datasets.

Timing and accuracy for each task was recorded. 
Time was measured manually, by means of a stopwatch. While stopwatches are hard to use for subsecond accuracy, this is not required for a study of this type. Errors in timing on the order of 5 seconds are acceptable.

For each task a date, time, source IP and where applicable username involved were recorded. This combined with the dataset provides sufficient information to allow checking of answers for accuracy from the raw logs, database directly, or using the tool. 

\subsection{Threats to validity}
 Students are less ideal, as their experience and domain knowledge are more limited than those that have been working in the industry for some time.
Dataset size - both in 10K order of magnitude, this limits our ability to speculate about how the tool will scale to 100K+ sizes. 

\section{Results}

Each Task was graded for accuracy, and time taken to complete the task was measured.
\begin{table}[tbh]
\centering
\begin{tabular}{l|*{6}{l|}}
Task & Participant 1 & Participant  2 & Participant  3 & Participant  4 & Participant 5 & Participant 6 \\
\hline
Task 1 & 3:05 & 5:46 & Invalid & Unfinished & Unfinished & Unfinished \\
Task 2 & Unfinished & Unfinished & 2:26 & 2:09 & 3:55 & 3:08 \\
Task 3 & 5:10 & Unfinished & 4:18 & 4:51 & 8:00 & 2:20 \\
Task 4 & Unfinished & Unfinished & 4:18 & 3:18 & Unfinished & Unfinished \\
Task 5 & 1:09 & 1:22 & 1:07 & 0:39 & 1:04 & 1:10 \\
Task 6 & 0:31 & 1:07 & 1:02 & 1:13 & 1:00 & 2:15 \\
Task 7 & 0:45 & 1:59 & 0:42 & 0:44 & 2:06 & Unfinished \\
Task 8 & 0:32 & 1:10 & 1:32 & 1:07 & 1:15 & 0:55 \\
\end{tabular}
\caption{Time taken to complete each tasks}
\label{res_times}
\end{table}

\begin{table}[tbh]
\centering
\begin{tabular}{l|*{6}{l|}}
Task & Participant 1 & Participant  2 & Participant  3 & Participant  4 & Participant 5 & Participant 6 \\
\hline
Task 1 & Correct & Incorrect & Incorrect & Incorrect & Incorrect & Incorrect\\
Task 2 & Incorrect & Incorrect & Correct & Correct & Correct & Correct\\
Task 3 & Correct & Incorrect & Correct & Correct & Correct & Incorrect\\
Task 4 & Incorrect & Incorrect & Correct & Incorrect & Incorrect & Incorrect\\
Task 5 & Correct & Correct & Correct & Correct & Correct & Correct\\
Task 6 & Correct & Correct & Correct & Incorrect & Correct & Correct\\
Task 7 & Correct & Correct & Incorrect & Correct & Correct & Incorrect\\
Task 8 & Correct & Incorrect & Incorrect & Correct & Correct & Correct\\
\end{tabular}
\caption{Accuracy for each task}
\label{res_acc}
\end{table}

Observation of participants working on tasks 1 through 4, and participant comments suggest that difficulty in navigating the timeline was a significant issue for carrying out these tasks.
Task 1 involved finding a brute force attack which compromised root on the ECS dataset. There was no such attack present in the dataset. It's well known that demonstrating the absence of something can be significantly harder than the presence.<justify this>. I believe the combination of navigational difficulties with increased task difficulty is the cause of the very high failure rate for this task, with only one participant successful.

Task 4 involved participants looking for a successful scattergun attack in the ECS dataset. why was this so hard? Poor navigation support caused problems, relatively small attack signature easily swamped in other data.

Tasks 2 \& 3 were to find a brute force and scattergun attack respectively on the Honeynet dataset.
There were multiple successful brute force attacks on the Honeynet dataset, and scattergun attacks with much larger attack signatures (higher number of attempts). These questions were quickly and reliably answered by all but one participant. Poor accuracy, and significantly slower times with ECS data coupled with observations of participants attempting these tasks suggest that navigation difficulties and limited filtering options were a greater issue in the more complicated dataset, with smaller attack signatures, and greater noise. 

Participant 2 had a great deal of difficulty in identifying brute force and scattergun attacks on both datasets. Feedback and observation of the participant in action suggest severe difficulties with navigation, combined with the lack of ability to hide all attempts from a specified set of IP addresses caused significant difficulties for this participant.

Participant 2 commented that in the normal course of investigating such incidents using tools such as grep, he would build up a blacklist of IP's to hide from results as they were fully investigated and discarded.
The tool as currently implemented does not support this. Participant 2 has significantly more experience in analysing sshd logs using traditional tools where stronger filtering tools are available, such as regular expressions. 

insert table of results here.. then do analysis after raw results shown and discussed.
note why time for participant 4 was discarded on one question (experimenter error.)
initial results suggest several improvements to be made
\begin{enumerate}
\item{Improved navigation through timeline. Users complain that it is easy to get lost when zooming. Animations may ease this transition. (possibly animate zooming in by expanding target bin to fill timeline, then chunking into new bins?)}
\item{Improved filtering of results by IP. several extra features are requested here, with the ability to filter by subnet, as well as hiding IP's (inverse of IP of Interest filter.)}
\item{Ability to filter connections by authtype, ie : excluding hostbased when looking for a successful brute force attack.}
\item{Improve performance of abnormal time and location detection, reduce false positives and spurious results by suppressing alerting on invalid or failed attempts. This will significantly cut down the number of abnormalities reported, reducing the false positive rate significantly.}
\end{enumerate}

include comparison with goals here.. how well did we meet them?
\begin{enumerate}
\item{Effective use of information hiding to prevent information overload.}
\item{Prevent masking of important data in noise or hiding. failed, more work required on filtering and clustering}
\item{Provide strong filtering and highlighting options. weaker than ideal, more work needed, performance dependent on exploration style and ability to maintain mental map of data.}
\item{Show surrounding context for anomalous accesses.}
\item{GeoIP support to add context to login attempts. geoIP results used in clustering algorithm, but not directly displayed.}
\item{Allow the user control over which machine is monitored at any given time. -included as dropdown selector, may have some scale issues for networks with many machines to monitor. no ability to select multiple machines currently, though dropdown easily expanded to allow this.}
\item{Show network context for currently monitored machine. dropped for lack of time}
\item{Performance capable of handling millions of events without perceptible delays. - untested, but unlikely due in part to network latency and javascript performance issues. did not have sufficiently large dataset to test performance with more than $10^3$ magnitude.}
\item{Extensible log parsing: don't prevent extension to other similar log types.}
\end{enumerate}

\section{Future work}
what do we need to do from here?
address known bugs.
address discovered weaknesses
 - add more filter types
 - add mechanism for excluding addresses from results.
 - refine clustering algorithm to reduce false positives. 
 - animate transitions to assist navigation.
maybe some work on performance?
critical - once bugs and new features addressed, perform a more complete evaluation
higher n, with actual statistical power, and much more variety of datasets. allow more training time, and some learning questions.