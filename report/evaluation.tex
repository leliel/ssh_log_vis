\chapter{Evaluation}\label{eval}

Evaluation of the tool produced in this project served to determine if there is merit to the idea justifying further development work on the tool. If the evaluation determines that there is potential to justify the development work, future work can evaluate the effectiveness of the design in a more detailed manner. I have chosen to perform an exploratory evaluation due to time limitations on this project \cite{Ellis:2006:EAU:1168149.1168152}. The exploratory evaluation takes the form of a small user study with two complimentary datasets. Results suggest that the approach taken with the tool is useful. 

Section \ref{res_study} discusses experimental methodology. Section \ref{res_res} discusses the results. 

\section{User Study}\label{res_study}

We performed a qualitative user study, with 6 expert participants. Two datasets were used, representing different sizes and complexities of system. Each participant was presented with 4 tasks to complete for each dataset. Time taken and accuracy was recorded for each task, along with a 3 question usability survey <ref appendix>. 
Ethics approval has been granted for this experimental design.<ref appendix> 

\subsection{Justification}
It is not possible to achieve a representative sample of sufficient size to have predictive power in the time available. Two factors limit the ability to achieve a representative sample in the time available, taking part in the experiment requires between 1 and 1.5 hours. a further 1 to 1.5 hours is required to grade and analyse the data. Further, there is a relatively limited pool of potential participants as the tool is targeted at security experts. This limitation strongly suggests against attempting to demonstrate conclusively the usefulness and usability of the system. Small, exploratory evaluations have advantages in cost and time required. With small numbers of participants, the evaluation can be carried out in a short time. These exploratory evaluations can also be useful to discard non-viable approaches before significant development effort has been spent. Care should be taken to avoid detailed, large scale evaluation too early however, as the unfinished nature of the product can distort results. 

\subsection{Users}

As this tool targets domain experts in the security domain, I have recruited participants from the security industry, both on campus and off. I have also recruited students who have passed NWEN405, which deals with computer security as an alternate source of participants. Students are a second preference for participants as they have significantly less experience in dealing with security threats in the real world. This tool is targeted for use by security professionals working in industry. <ref personae (appendix)> However this lack of experience can be an advantage, as students are not yet so accustomed to a particular approach to log analysis. 
All four professional participants had security issues as at least a portion of their day jobs, and could be expected to be involved in log analysis tasks on a regular basis. 3 of the professional participants are employed by the Victoria University School of Engineering. 2 students. 

\begin{table}[tbh]
\centering
\begin{tabular}{l|*{6}{l|}}
& \multicolumn{6}{|c|}{Participant} \\
& 1 & 2 & 3 & 4 & 5 & 6 \\
Professional & \cmark & \cmark & & & \cmark & \cmark \\
Student & & & \cmark & \cmark & & \\ 
\end{tabular}
\caption{Participant number and role}
\label{res_parts}
\end{table}

\subsection{Datasets}\label{data}

Two datasets have been acquired for use in this experiment. Both datasets represent real systems exposed to the public internet. 
\begin{enumerate}
\item{Honeynet Forensic challenge 10 dataset \cite{forensic10}. This dataset is anonymised and public domain. The data presented here covers a single server, with 35K log entries covering March 16 to May 2}
\item{Anonymised logs from the Engineering and Computer Science network at Victoria University. This dataset covers three servers, for two disjoint weeks and 74K log entries.}
\end{enumerate}

The Honeynet dataset has been extensively analysed, over the course of two forensic challenges. Multiple successful brute force and scattergun attacks have been identified in this dataset. There were few usernames which showed a pattern of usage in the log data gathered. 

The ECS dataset is significantly more complex than the Honeynet dataset. This data covers three seperate servers, with significantly more active users. The ECS dataset contains disconnection messages absent from the Honeynet dataset. These messages add a significant amount of noise. This dataset was extensively analysed before the experiment to search for existing attacks. Several attempted brute force and scattergun attacks were found, though all were determined to be unsuccessful.  

The difference in attack success rates between datasets reflects differences in the purposes of the networks. The ECS network is provided for use by staff and students at the school of engineering and computer science at victoria university. This network can reasonably be expected to have significant amounts of sensitive information stored. Therefore the school expends significant effort in protecting these systems. In contrast, the Honeynet system is deliberately exposed to lure in attackers, and as such has significantly less effort expended on securing it.

While both datasets were anonymized, anonymization procedures were different. The Honeynet dataset is publically available in an anonymized form \cite{forensic10} whereas the ECS dataset was recorded from internet facing servers in the ECS network. The latter dataset required anonymization of both usernames and IP addresses. IP addresses were anonymized with cryptoPAN <ref> a prefix preserving IP address anonymization tool extended with support for IPv6 addresses. Usernames were anonymized through a very simple scheme, where each username was replaced with the string "user" + a unique number. ie: user4 is different from user5.

The ECS dataset was altered, to introduce a successful scattergun attack on one server. ThIs was introduced, as there were no naturally occuring successful attacks discovered after thorough analysis with both the tool, and direct exploration through the database.  
The introduced scattergun attack in the ECS dataset had a relatively small attack signature, with only 204 log entries involved, of 4.7K entries for that day. Some successful attacks on honeynet had similar numbers of access attempts, though often a much higher proportion of invalid or failed attempts for a given day. 

\subsection{Setup}
Participants were provided with one of two machines
\begin{itemize}
\item{Dell Optiplex 9010 with an i7-3770 CPU and 8Gb of ram running Arch linux 3.7.5}
\item{Dell Optiplex GX760 with a Core2 Duo CPU and 4GB ram, running Arch linux 3.7.5}
\end{itemize}
Both systems had chrome 26.0.1410.63 as the browser.

Different hardware was used, as I was not able to secure the same room for all participants. 
While there are significant differences in the hardware capabilities, I do not believe that these are significant
as the design of the tool does all memory and computationally intensive work server side, with only a few kilobytes of data transferred for each view. This results in a relatively small memory footprint, and limited resource requirements for the client computer.
More significant is the consistent OS and browser versions, as javascript behaviour can be influenced strongly by browser implementation. Though this is relatively rare for javascript behaviour to change significantly between major browser versions, this is not unknown. By ensuring both systems used the same OS and browser, this possible source of error is eliminated.

The experiment was conducted in a quiet lab, with only the experimenter and subject present. Each question was allocated an 8 minute maximum, based on test runs with a supervisor. Audio recordings were made as a record of events during the experiment as an addition to handwritten notes. 

\subsection{Procedure}
Participants were given up to ten minutes to familiarise themselves with the tool, and how it behaves. After familiarisation, participants were asked to answer four questions about each  dataset for a total of 8 tasks. Tasks were presented to users in randomised order. This was done in order to avoid learning effects distorting results. For each of the 8 tasks a brief questionnaire was completed indicating opinions about the tool's performance in the task\cite{lewis1995ibm}. As users are not given the opportunity to read questions before the experiment begins there is limited opportunity to learn the answers to later questions while performing a task. 

\begin{enumerate}
\item{Find an instance of a successful brute force attack on root.}
\item{Find an instance of a successful scattergun attack.
(an instance where the attacker attempts many common username/password pairs at random).}
\item{Find an instance of a legitimate user logging in from an abnormal location.}
\item{Find an instance of a legitimate user logging in at an abnormal time.}
\end{enumerate}

Questions 1 \& 2 are based on the most commonly found attack signatures in SSH logs. with many botnets and automated systems carrying out brute force or scattergun attacks against any IP address responding to connection requests. As these attacks are very common and can lead to serious compromises, as shown in the Honeynet dataset, determining success or failure of such attacks is a core function of any log analysis tool. 

Questions 3 \& 4 are based in finding anomalous behaviour by legitimate accounts. Anomalous behaviour by legitimate accounts can be an indication that their account has been compromised, or that the account owner has become malicious. <Badly worded. I mean they might be trying to compromise the system or data stored there.>. 

Tasks 1 through 4 were based on questions 1 \& 2, with tasks 1 \& 4 using the ECS dataset, and tasks 2 \&3 using the Honeynet dataset.
Tasks 5 through 8 were based on questions 3 \& 4, with tasks 5 \& 7 using the ECS dataset, and tasks 6 \& 8 using  the Honeynet dataset. 

Timing and accuracy for each task was recorded. 
Time was measured manually, by means of a stopwatch. While stopwatches are hard to use for subsecond accuracy, this is not required for a study of this type. Errors in timing on the order of 5 seconds are acceptable.

For each task a date, time, source IP and where applicable username involved were recorded. This combined with the dataset provides sufficient information to allow checking of answers for accuracy from the raw logs, database directly, or using the tool. 

\subsection{Threats to validity}
 Students are less ideal, as their experience and domain knowledge are more limited than those that have been working in the industry for some time.
 
Both datasets are in the 10's of thousands of entries, This has prevented any investigation of how the tool scales to much larger networks with 100K+ log entries in a similar time period. However, the ECS dataset covers a much shorter time period, with many more entries. This provides some indication of how performance scales with log density.
 
The extremely small number of participants means that the sample cannot be known to be representative. This greatly limits our ability to extrapolate to users outside of the sample. <work in a green grids reference here. Not quite sure how to word it yet.> 

The addition of a successful scattergun attack to the ECS dataset represents a potential weakness of this study, however great care was taken to ensure that the inserted log entries matched the patterns found in other scattergun attacks on both datasets. 

\section{Results}\label{res_res}

Each Task was given a pass/fail grade based on accuracy. Time taken to complete the task was also measured. Results are shown in Table \ref{res_times} A dash indicates the task was not completed in time, and is an automatic fail. Invalid marks a task where experimenter error spoiled results.
\begin{table}[tbh]
\centering
\begin{tabular}{l|*{12}{l|}}
Task & 
\multicolumn{2}{|c|}{Participant 1} & 
\multicolumn{2}{|c|}{Participant 2} & 
\multicolumn{2}{|c|}{Participant 3} & 
\multicolumn{2}{|c|}{Participant 4} & 
\multicolumn{2}{|c|}{Participant 5} & 
\multicolumn{2}{|c|}{Participant 6} \\
\hline
Task 1 & 3:05 & \cmark & 5:46 & \xmark & Invalid & \xmark & - & \xmark & - & \xmark & - & \xmark \\
Task 2 & - & \xmark & - & \xmark & 2:26 & \cmark & 2:09 & \cmark & 3:55 & \cmark & 3:08 & \cmark \\
Task 3 & 5:10 & \cmark & - & \xmark & 4:18 & \cmark & 4:51 & \cmark & 8:00 & \cmark & 2:20 & \xmark \\
Task 4 & - & \xmark & - & \xmark & 4:18 & \cmark & 3:18 & \xmark & - & \xmark & - & \xmark\\
Task 5 & 1:09 & \cmark & 1:22 & \cmark & 1:07 & \cmark & 0:39 & \cmark & 1:04 & \cmark & 1:10 & \cmark \\
Task 6 & 0:31 & \cmark & 1:07 & \cmark & 1:02 & \cmark & 1:13 & \xmark & 1:00 & \cmark & 2:15 & \cmark \\
Task 7 & 0:45 & \cmark & 1:59 & \cmark & 0:42 & \xmark & 0:44 & \cmark & 2:06 & \cmark & - & \xmark \\
Task 8 & 0:32 & \cmark & 1:10 & \xmark & 1:32 & \xmark & 1:07 & \cmark & 1:15 & \cmark & 0:55 & \cmark \\
\end{tabular}
\caption{Time and accuracy results for each task.}
\label{res_times}
\end{table}

Observation of participants working on tasks 1 through 4, and participant comments suggest that difficulty in navigating the timeline was a significant issue for carrying out these tasks.
Task 1 involved finding a brute force attack which compromised root on the ECS dataset. There was no such attack present in the dataset. It's well known that demonstrating the absence of something can be significantly harder than the presence.<justify this>. I believe the combination of navigational difficulties with increased task difficulty is the cause of the very high failure rate for this task, with only one participant successful.

Task 4 involved participants looking for a successful scattergun attack in the ECS dataset. why was this so hard? Poor navigation support caused problems, relatively small attack signature easily swamped in other data.

Tasks 2 \& 3 were to find a brute force and scattergun attack respectively on the Honeynet dataset.
There were multiple successful brute force attacks on the Honeynet dataset, and scattergun attacks with much larger attack signatures (higher number of attempts). These questions were quickly and reliably answered by all but one participant. Poor accuracy, and significantly slower times with ECS data coupled with observations of participants attempting these tasks suggest that navigation difficulties and limited filtering options were a greater issue in the more complicated dataset, with smaller attack signatures, and greater noise. 

The introduced scattergun attack in the ECS dataset had a relatively small attack signature, with only 204 log entries involved, of 4.7K entries for that day. Some successful attacks on honeynet had similar numbers of access attempts, though often a much higher proportion of invalid or failed attempts for a given day. 

There were many more legitimate access attempts to the ECS network, and logging of disconnect messages introduced further noise to the system. 
<check actual results, some successful root attacks on honeynet had similar sized signatures, some much larger, though with more noise for larger datasets.>
this needs more analysis.

Participant 2(Professional, see Table \ref{res_parts}) had a great deal of difficulty in identifying brute force and scattergun attacks on both datasets. Feedback and observation of the participant in action suggest severe difficulties with navigation, combined with the lack of ability to hide all attempts from a specified set of IP addresses caused significant difficulties for this participant.

Participant 2 commented that in the normal course of investigating such incidents using tools such as grep, he would build up a blacklist of IP's to hide from results as they were fully investigated and discarded.
The tool as currently implemented does not support this. Participant 2 has significantly more experience in analysing sshd logs using traditional tools where stronger filtering tools are available, such as regular expressions. 

\section{Discussion} 

Analysis of the results of this user evaluation suggest that there are several obvious weaknesses in the tool as currently implemented. 

These break down into three major areas; Navigation; Filtering; and Information hiding.
Navigation difficulties were experienced by a majority of users, where abrubt transitions between zoom levels lead to loss of context, and difficulty building up a mental map of the timeline. 
Most users demonstrated improved navigation as they became more familiar with the tool. these issues could be addressed in two major ways. Showing a miniature zoomed out view as a context for the main view could assist users in maintaining context. 
Animating transitions may also assist users to build up a mental map of the log. Improvements to navigation would support nonfunctional requirements 1 through 3 (\ref{reqs}). 

Analysis of the difficulties experienced by participant 2, and suggestions from other participants suggest several new filtering options would significantly enhance ability to deal with potential information overload. 
Implementing IP blacklisting would be extremely useful for some users, as it would support an interaction model where one IP is fully investigated, then hidden, and the process repeated until all suspicious IP's have been investigated. 

One participant suggested allowing filtering by authentication method. This is strongly supported by the difficulties users had in Task 1, as on multiple occasions, a successful root login would occur mixed in with many failed attempts. This successful login would be from a different IP address, using an authentication method not amenable to brute forcing, such as host-based authentication. Allowing filtering by authentication method, would assist users in avoiding this pitfall. by hiding a class of logins which cannot be involved in such an attack. This would better support both functional and nonfunctional requirements (\ref{reqs}).

\subsection {Suggested Improvements}
Results suggest the following concrete improvements could be made to the tool, to better meet functional and nonfunctional requirements. 

\begin{enumerate}
\item{Animated zooming. Users complain that it is easy to get lost when zooming. A common tool used to smooth out such transitions and ease navigation is animation of the zoom.}
\item{Filtering by subnet. This would allow more flexible filtering by IP address. Currently filters are restricted to matching against dotted quad forms of address.}
\item{IP Hiding. the inverse of the IP of interest filter, showing all addresses except those selected. This would allow support of another interaction model, as used by participant 2}
\item{Filter events by authentication type. This will help reduce false detection rates for brute force and similar attacks, as some authentication methods are not vulnerable to these attacks.}
\item{Suppress abnormal time and place warnings for invalid and failed attempts. This will significantly cut down the number of abnormalities reported, reducing the false positive rate significantly. This will also help reduce information overload from this flag.}
\item{Seperate disconnection messages into their own type. This would allow hiding of disconnection messages in the same way failed and successful connections can be hidden, reducing noise in the dataset.}
\end{enumerate}

\subsection{Functional Requirements}
Several functional requirements were laid out for this tool in Chapter \ref{reqs}
How well have these requirements been met by the tool?

\begin{enumerate}
\item{Strong filtering and highlighting options. These require some extension to meet the needs of users, However, most users were able to successfully answer questions about both datasets with existing tools. Highlighting is not currently implemented.}
\item{Show surrounding context for anomalous accesses. The timeline display always shows surrounding time, with each level of zoom reducing the surrounding time that is visible. This provides context to users, but requires improvement due to navigational difficulties bought on by rapid transitions.}
\item{GeoIP support to add context to login attempts. This is currently used for abnormal location detection, but is not currently made available to the user directly.}
\item{Allow the user control over which machine is monitored at any given time. Users have direct control over which machine's log data is shown at any time through the server dropdown, which is automatically populated with a full list of all servers known to the database.}
\item{Show network context for currently monitored machine. While this was part of the design, there was not sufficient time to implement this feature.}
\item{Extensible log parsing:  is currently limited to SSHD logs, though could easily be extended to cope with any log using the syslog format. The database as currently structured is only suitable for SSHD logs. Any SSHD log containing the appropriate metadata could be parsed into a form suitable for storage in the database, Though this would require a customized parser implemtation. <Should this even really be a requirement? it's a bit out of place> }
\end{enumerate}

\subsection{Non-functional Requirements}
Several non-functional requirements were developed for this tool, to deal with issues of scaling and information overload.

\begin{enumerate}
\item{Use of information hiding to prevent information overload was well enough implemented to allow users to quickly identify features of the dataset in simpler cases. This needs further work to be considered met.}
\item{Prevent masking of important data.}
\item{Navigation; as currently implemented, navigation can pose significant issues for users, this goal cannot be considered as met, However most users improved over time with the tool.}
\item{Performance capable of handling millions of events. Scaling was not tested for more than 75K log entries. Extrapolation from ECS and Honeynet datasets is difficult, as the ECS dataset was both larger, and significantly more complex}
\end{enumerate}

\section{Future work}
How do we go about addressing the weaknesses discussed above?.
can they be addresssed?
address known bugs.
address discovered weaknesses
 - add more filter types
 - add mechanism for excluding addresses from results.
 - refine clustering algorithm to reduce false positives. 
 - animate transitions to assist navigation.
maybe some work on performance?
critical - once bugs and new features addressed, perform a more complete evaluation
higher n, with actual statistical power, and much more variety of datasets. allow more training time, and some learning questions.