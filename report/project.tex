\chapter{Project Methodology}\label{proj}

As formal project management methodologies are designed to control projects with many workers, they have an unacceptable time overhead in producing required documentation and tracking progress. Agile methodologies are all designed to apply to projects with multiple workers, and do not apply well to a single person project. As such, no formalised project management technique was used in this project.  
Weekly meetings were held with my supervisors, to track progress of the project, and User centered design was applied throughout the project. 

The project took shape in 4 major sections, which this report is structured to reflect. Background reading was done first, to develop a deeper understanding of the problem. This was followed by design work. The design and implementation phases were not distinctly seperated, with many design decision deferred until implementation. <why?> After implementation was frozen, a small scale, exploratory user evaluation was carried out.


Several supporting tools were used extensively to support the implementation of the tool, these will be discussed later in this chapter. 

\section{User Personae}

This tool is aimed at two major groups of users, frontline network security professionals and their managers.
Both may be required to use the results of log analyses in the normal course of their work. Also of concern in the design process are hackers and other malicious users, an anti-persona has been developed to model this group. <Ref appendices with full personae> As there are two main groups of users, two personae were developed to model these users. 
The primary user group I envisage for this tool are frontline network security professionals, as they can be expected to be the primary investigators for any issues, and tasked with routine monitoring of the network.
Security managers are included as a secondary user group as the frontline security personel would be required to report security issues to their managers.

\section{Requirements}\label{reqs}

In order to meet the goals laid out in Chapter \ref{C:intro}, several concrete requirements have been laid out for the tool. These requirements are divided into functional and non-functional requirements. Requirements 1 through 3 and 6 support goals 1 and 2. Requirements 3 through 7 support goal 3. Scaling goals are supported by all requirements, as goals 1 and 2 strongly support scaling to millions of events.  

\begin{enumerate}
\item{Strong filtering and highlighting options.}
\item{Show surrounding context for anomalous accesses.}
\item{GeoIP support to add context to login attempts.}
\item{Allow the user control over which machine is monitored at any given time.}
\item{Show network context for currently monitored machine.}
\item{Extensible log parsing: don't prevent extension to other similar log types.}
\end{enumerate}

Non functional requirements deal with issues of performance, scalability and usability.
\begin{enumerate}
\item{Use information hiding to prevent information overload.}
\item{Prevent masking of important data.}
\item{Navigation: tool needs to support easy navigation of the timeline.}
\item{Performance capable of handling millions of events.}
\end{enumerate}

\section{Ethical Considerations}

This project is not entirely without ethical concerns, as while the tool serves to aid understanding of information already available to network security personel. The user evaluation carries some ethical risks, which must be handled.
There are two groups of ethical concern arising from the user evaluation. The first is caused by the nature of the datasets used.  The second set of ethical concerns arise from the user study itself.

\subsection{Datasets}
The ECS dataset (see \ref{data}) is recorded from a live network in normal operation. The log produced contains IP addresses and usernames of everyone to use the system during the recording period. As this information could easily be used to identify people, it requires anonymization to be ethically used without each user's consent. SSHD logs contain usernames and raw IP addresses. Both usernames and IP addresses can be useful to malicious persons. IP addresses allow for approximate location of a user's home through GeoIp databases, as well as direct attacks on the security of their computers. Usernames do not carry as significant a risk to the owner, though these could be useful to mount an attack on the ECS system directly. 

Anonymization was achieved through the use of a modified version of the cryptoPAN<ref> library, and a custom script to replace usernames with unique codes. CryptoPAN uses strong encryption to generate codes which are combined with the IP address to produce a new valid IP address. This cannot be reversed without knowlege of the key, or an efficient means of cracking AES encryption. Username de-anonymization would require knowlege of the order users logged in on each server throughout the logged period. This knowlege is impractical to obtain. 

The Honeynet dataset publically available. No futher anonymization is required for this dataset due to public availablity. if it's not properly anonymized, no further harm will be done. 

\subsection{Evaluation procedures}
Fairly standard for user evaluations. Privacy concerns, can't reveal identity of participants -> fear of repercussions can distort data, confidentiality increases openness (doubly so for students). 
Use numbers to refer to participants. destroy data after report completed. 

\section{Supporting tools}
Several supporting tools were used during the implementation and later stages of the project for source code management and version control; Issue tracking software was used in the later half of the implementation stage to track bugs and new features.

\subsection{Version Control}
I chose to use a distributed version control system for source control, as this serves a dual purpose. As each instance of a distributed repository contains the whole history, and all of the data, the repository also serves as a distributed backup system. 
I chose to use git as the distributed version control system, as it is one that I was already familiar with from earlier projects. The distributed nature of git saved me several times, as I managed to accidentally corrupt the local copy of the repository. This could be resolved very easily by simply deleting the local copy, and replacing with a clone from either remote repository. In the worst case, I lost one day's progress, through user error.

\subsection{GitHub}
GitHub <make reference> is a website which offers public and private git repositories. public repositories are free, and come with integrated issue tracking software capable of linking commits to each issue, and closing issues from a commit message. (Git does not automatically update the local repository every time a file is changed. This must be done manually and is called a commit. git requires that a message be attached to all commits, usually used to explain the purpose of the code changes). The issue tracking software made available through github proved invaluable for keeping track of bugs that were discovered, and new features that were added. The issue tracker also allowed linking of commits against issues, this was useful for tracking what changes were involved in each bugfix.

\subsection{Stack Overflow}
As several new technologies had to be learned in the process of implementing the tool, I made extensive use of online documentation. of partictular note for this is Stack overflow; a  Q\&A format forum for developers. Stack overflow has grown to be an excellent reference for practical advice on development problems across a wide range of languages and technologies, with some excellent and well researched answers. However, as with any forum, a good ability to judge plausibility is required, as there are also many misleading or wrong answers. However the feedback tools available to users quickly seperate the good answers from the rest. As stack overflow is searchable, I was easily able to find answers for several perplexing issues on this site. 